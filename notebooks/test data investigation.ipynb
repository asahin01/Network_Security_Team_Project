{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rationale\n",
    "It is suspicious that models with a high (>98%) accuracy on cross validation with the training set should score barely better than chance (52-54%) on the test set.  From an initial comparison of the two data sets it appears that for some features the values in the tests set are considerably different from the values in the training set.  Prediction is only possible if both the training set and test set are drawn from the same underlying population and have features that have been identically processed.\n",
    "\n",
    "This notebook investigates the hypothesis that there is some systematic bias or difference between the two data sets that makes accurate prediction impossible.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_imperson_without4n7_balanced_data.csv')\n",
    "X_train, Y_train = train_data.loc[:, train_data.columns != '155'], train_data['155']\n",
    "\n",
    "test_data = pd.read_csv('test_imperson_without4n7_balanced_data.csv')\n",
    "X_test, Y_test = test_data.loc[:, test_data.columns != '155'], test_data['155']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the columns with no variance and check they are the same for both data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_no_var = X_train.var() == 0.0\n",
    "test_no_var = X_test.var() == 0.0\n",
    "all(train_no_var == test_no_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so the columns in the training set with no variance are different from the columns in the test set with no variance.  Let investigate them further..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>69</th>\n",
       "      <th>81</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>86</th>\n",
       "      <th>88</th>\n",
       "      <th>97</th>\n",
       "      <th>113</th>\n",
       "      <th>117</th>\n",
       "      <th>133</th>\n",
       "      <th>138</th>\n",
       "      <th>148</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>97044.0</td>\n",
       "      <td>97044.0</td>\n",
       "      <td>97044.000000</td>\n",
       "      <td>97044.000000</td>\n",
       "      <td>97044.000000</td>\n",
       "      <td>97044.000000</td>\n",
       "      <td>97044.00000</td>\n",
       "      <td>97044.000000</td>\n",
       "      <td>97044.000000</td>\n",
       "      <td>97044.00000</td>\n",
       "      <td>97044.000000</td>\n",
       "      <td>97044.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.500004</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004540</td>\n",
       "      <td>0.007863</td>\n",
       "      <td>0.009079</td>\n",
       "      <td>0.002451</td>\n",
       "      <td>0.00321</td>\n",
       "      <td>0.002383</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.00321</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.428570</td>\n",
       "      <td>0.156110</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            69       81            83            84            86  \\\n",
       "count  97044.0  97044.0  97044.000000  97044.000000  97044.000000   \n",
       "mean       0.0      0.0      0.000021      0.000062      0.000082   \n",
       "std        0.0      0.0      0.004540      0.007863      0.009079   \n",
       "min        0.0      0.0      0.000000      0.000000      0.000000   \n",
       "25%        0.0      0.0      0.000000      0.000000      0.000000   \n",
       "50%        0.0      0.0      0.000000      0.000000      0.000000   \n",
       "75%        0.0      0.0      0.000000      0.000000      0.000000   \n",
       "max        0.0      0.0      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 88           97           113           117          133  \\\n",
       "count  97044.000000  97044.00000  97044.000000  97044.000000  97044.00000   \n",
       "mean       0.000014      0.00001      0.000013      0.000006      0.00001   \n",
       "std        0.002451      0.00321      0.002383      0.000911      0.00321   \n",
       "min        0.000000      0.00000      0.000000      0.000000      0.00000   \n",
       "25%        0.000000      0.00000      0.000000      0.000000      0.00000   \n",
       "50%        0.000000      0.00000      0.000000      0.000000      0.00000   \n",
       "75%        0.000000      0.00000      0.000000      0.000000      0.00000   \n",
       "max        0.468750      1.00000      0.428570      0.156110      1.00000   \n",
       "\n",
       "                138      148  \n",
       "count  97044.000000  97044.0  \n",
       "mean       0.500004      0.0  \n",
       "std        0.001220      0.0  \n",
       "min        0.500000      0.0  \n",
       "25%        0.500000      0.0  \n",
       "50%        0.500000      0.0  \n",
       "75%        0.500000      0.0  \n",
       "max        0.880000      0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[X_train.columns[train_no_var != test_no_var]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>69</th>\n",
       "      <th>81</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>86</th>\n",
       "      <th>88</th>\n",
       "      <th>97</th>\n",
       "      <th>113</th>\n",
       "      <th>117</th>\n",
       "      <th>133</th>\n",
       "      <th>138</th>\n",
       "      <th>148</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40158.000000</td>\n",
       "      <td>40158.000000</td>\n",
       "      <td>40158.0</td>\n",
       "      <td>40158.0</td>\n",
       "      <td>40158.0</td>\n",
       "      <td>40158.0</td>\n",
       "      <td>40158.0</td>\n",
       "      <td>40158.0</td>\n",
       "      <td>40158.0</td>\n",
       "      <td>40158.0</td>\n",
       "      <td>40158.0</td>\n",
       "      <td>40158.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.463320</td>\n",
       "      <td>0.042274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.498659</td>\n",
       "      <td>0.068034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.272730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 69            81       83       84       86       88  \\\n",
       "count  40158.000000  40158.000000  40158.0  40158.0  40158.0  40158.0   \n",
       "mean       0.463320      0.042274      0.0      0.0      0.0      0.0   \n",
       "std        0.498659      0.068034      0.0      0.0      0.0      0.0   \n",
       "min        0.000000      0.000000      0.0      0.0      0.0      0.0   \n",
       "25%        0.000000      0.000000      0.0      0.0      0.0      0.0   \n",
       "50%        0.000000      0.000000      0.0      0.0      0.0      0.0   \n",
       "75%        1.000000      0.090909      0.0      0.0      0.0      0.0   \n",
       "max        1.000000      0.272730      0.0      0.0      0.0      0.0   \n",
       "\n",
       "            97      113      117      133      138           148  \n",
       "count  40158.0  40158.0  40158.0  40158.0  40158.0  40158.000000  \n",
       "mean       0.0      0.0      0.0      0.0      0.0      0.000025  \n",
       "std        0.0      0.0      0.0      0.0      0.0      0.004990  \n",
       "min        0.0      0.0      0.0      0.0      0.0      0.000000  \n",
       "25%        0.0      0.0      0.0      0.0      0.0      0.000000  \n",
       "50%        0.0      0.0      0.0      0.0      0.0      0.000000  \n",
       "75%        0.0      0.0      0.0      0.0      0.0      0.000000  \n",
       "max        0.0      0.0      0.0      0.0      0.0      1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[X_test.columns[train_no_var != test_no_var]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what the actual values are in these columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'69': [(0, 97044)],\n",
       " '81': [(0, 97044)],\n",
       " '83': [(0, 97042), (1, 2)],\n",
       " '84': [(0, 97038), (1, 6)],\n",
       " '86': [(0, 97036), (1, 8)],\n",
       " '88': [(0.0, 97038), (0.03125, 3), (0.46875, 2), (0.375, 1)],\n",
       " '97': [(0, 97043), (1, 1)],\n",
       " '113': [(0.0, 97041), (0.42857, 3)],\n",
       " '117': [(0.0, 97036),\n",
       "  (0.0034351, 3),\n",
       "  (0.15611, 2),\n",
       "  (0.11412, 1),\n",
       "  (0.097328, 1),\n",
       "  (0.096183, 1)],\n",
       " '133': [(0, 97043), (1, 1)],\n",
       " '138': [(0.5, 97043), (0.88, 1)],\n",
       " '148': [(0, 97044)]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    col: list(X_train[col].value_counts().items()) \n",
    "    for col in X_train[X_train.columns[train_no_var != test_no_var]].columns\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'69': [(0, 21552), (1, 18606)],\n",
       " '81': [(0.0, 27711),\n",
       "  (0.090909, 6243),\n",
       "  (0.18182, 6181),\n",
       "  (0.27273000000000003, 23)],\n",
       " '83': [(0, 40158)],\n",
       " '84': [(0, 40158)],\n",
       " '86': [(0, 40158)],\n",
       " '88': [(0, 40158)],\n",
       " '97': [(0, 40158)],\n",
       " '113': [(0, 40158)],\n",
       " '117': [(0, 40158)],\n",
       " '133': [(0, 40158)],\n",
       " '138': [(0, 40158)],\n",
       " '148': [(0, 40157), (1, 1)]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{col: list(X_test[col].value_counts().items()) \n",
    "    for col in X_test[X_test.columns[train_no_var != test_no_var]].columns\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we can see that there are 9 features that have variance in the training set but no variance in test set, and three features the other way round.  For most of the cases the columns with variance only have a single value or handful of values that differ, so are unlikely to have much effect on any model trained on the data.\n",
    "\n",
    "We shall exclude all these features from further investigation, along with the features that have no variance for either data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((97044, 69), (40158, 69))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_with_var = X_train[X_train.columns[~(train_no_var | test_no_var)]]\n",
    "X_test_with_var = X_test[X_test.columns[~(train_no_var | test_no_var)]]\n",
    "\n",
    "X_train_with_var.shape, X_test_with_var.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By eliminating features that have no variance in either the training or test set (or both) we are left with 69 features.  \n",
    "\n",
    "We now want to test for each feature in the remaining data sets whether the feature in the test set comes from the same distribution as the same feature in the training set.  We shall use the ks2_samp function to determine this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5': 0.0,\n",
       " '6': 0.0,\n",
       " '8': 0.0,\n",
       " '9': 0.0,\n",
       " '14': 1.0,\n",
       " '15': 1.0,\n",
       " '16': 1.0,\n",
       " '18': 1.0,\n",
       " '20': 1.0,\n",
       " '26': 1.0,\n",
       " '29': 1.0,\n",
       " '38': 0.0,\n",
       " '43': 1.0,\n",
       " '47': 4.854342014699145e-109,\n",
       " '48': 1.0,\n",
       " '50': 2.1319088282967322e-89,\n",
       " '51': 3.7396694731191775e-89,\n",
       " '52': 1.0,\n",
       " '61': 0.0,\n",
       " '62': 1.0,\n",
       " '64': 1.0315072609922148e-131,\n",
       " '66': 7.746639575795464e-91,\n",
       " '67': 3.503944077968135e-64,\n",
       " '68': 4.51926145509109e-49,\n",
       " '70': 7.306613166889381e-41,\n",
       " '71': 0.0,\n",
       " '72': 0.995487532890464,\n",
       " '73': 2.810115497553139e-53,\n",
       " '75': 0.0,\n",
       " '76': 0.0,\n",
       " '77': 0.0,\n",
       " '78': 0.0,\n",
       " '79': 0.0,\n",
       " '80': 0.0,\n",
       " '82': 0.0,\n",
       " '89': 1.0,\n",
       " '90': 0.3461723270023699,\n",
       " '93': 2.3612312065919717e-20,\n",
       " '94': 0.9999999999999997,\n",
       " '98': 0.4145970321873381,\n",
       " '104': 0.8784705620648641,\n",
       " '105': 1.0,\n",
       " '106': 1.0,\n",
       " '107': 3.2841854096356005e-09,\n",
       " '108': 2.5745299377723217e-210,\n",
       " '109': 0.8014300689769817,\n",
       " '110': 2.0290749144082378e-94,\n",
       " '111': 1.0,\n",
       " '112': 0.478879497860954,\n",
       " '118': 0.43558042845336914,\n",
       " '119': 4.06200452902175e-200,\n",
       " '120': 3.9226357263333426e-05,\n",
       " '121': 0.10549100404399775,\n",
       " '122': 1.0466202709142244e-57,\n",
       " '123': 1.0,\n",
       " '125': 0.5279903107229729,\n",
       " '126': 9.429153389711992e-25,\n",
       " '127': 9.429153389711992e-25,\n",
       " '128': 9.429153389711992e-25,\n",
       " '129': 9.429153389711992e-25,\n",
       " '130': 0.007950015782689166,\n",
       " '140': 0.0,\n",
       " '141': 0.0,\n",
       " '142': 0.0,\n",
       " '143': 0.0935525709123911,\n",
       " '144': 1.1860388024364577e-16,\n",
       " '145': 0.9871507526074228,\n",
       " '146': 0.9871507526074228,\n",
       " '154': 0.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks_pvalues = {col: stats.ks_2samp(X_train[col], X_test[col]).pvalue \n",
    "    for col in X_train_with_var.columns\n",
    "}\n",
    "ks_pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in ks_pvalues.values() if x < 0.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High p-values mean that column probably comes from the same distribution, low pvalues mean they probably come from a different distribution, at least as far as the Kolmogorov-Smirnov test can tell.  From the above you can see that 40 of the 69 features have a p-value of less than 0.05 and many of them have a p-value of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets focus on the features that our team identified as being the most important for training the model.  We found that just six features could be used to create a model that scored in excess of 98% in cross validation on the training data, but barely more than 52% on the test data.  It turns out that all these features score close to zero p-value on the KS test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('8', 0.0),\n",
       " ('38', 0.0),\n",
       " ('67', 3.503944077968135e-64),\n",
       " ('76', 0.0),\n",
       " ('78', 0.0),\n",
       " ('119', 4.06200452902175e-200)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_features = ['8', '38', '67', '76', '78', '119']\n",
    "[(f, ks_pvalues[f]) for f in best_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>8</th>\n",
       "      <th>38</th>\n",
       "      <th>67</th>\n",
       "      <th>76</th>\n",
       "      <th>78</th>\n",
       "      <th>119</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>97044.000000</td>\n",
       "      <td>97044.000000</td>\n",
       "      <td>97044.000000</td>\n",
       "      <td>97044.000000</td>\n",
       "      <td>97044.000000</td>\n",
       "      <td>97044.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.193837</td>\n",
       "      <td>0.713756</td>\n",
       "      <td>0.376979</td>\n",
       "      <td>0.003294</td>\n",
       "      <td>0.002077</td>\n",
       "      <td>0.000336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.354444</td>\n",
       "      <td>0.109809</td>\n",
       "      <td>0.395372</td>\n",
       "      <td>0.021005</td>\n",
       "      <td>0.017849</td>\n",
       "      <td>0.011518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.037908</td>\n",
       "      <td>0.654075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.037908</td>\n",
       "      <td>0.757210</td>\n",
       "      <td>0.307690</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.767550</td>\n",
       "      <td>0.615380</td>\n",
       "      <td>0.002253</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933540</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990990</td>\n",
       "      <td>0.995670</td>\n",
       "      <td>0.994750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  8            38            67            76            78  \\\n",
       "count  97044.000000  97044.000000  97044.000000  97044.000000  97044.000000   \n",
       "mean       0.193837      0.713756      0.376979      0.003294      0.002077   \n",
       "std        0.354444      0.109809      0.395372      0.021005      0.017849   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.037908      0.654075      0.000000      0.000563      0.001731   \n",
       "50%        0.037908      0.757210      0.307690      0.000563      0.001731   \n",
       "75%        0.054902      0.767550      0.615380      0.002253      0.001731   \n",
       "max        1.000000      0.933540      1.000000      0.990990      0.995670   \n",
       "\n",
       "                119  \n",
       "count  97044.000000  \n",
       "mean       0.000336  \n",
       "std        0.011518  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        0.994750  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[best_features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>8</th>\n",
       "      <th>38</th>\n",
       "      <th>67</th>\n",
       "      <th>76</th>\n",
       "      <th>78</th>\n",
       "      <th>119</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40158.000000</td>\n",
       "      <td>40158.000000</td>\n",
       "      <td>40158.000000</td>\n",
       "      <td>40158.000000</td>\n",
       "      <td>40158.000000</td>\n",
       "      <td>40158.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.200639</td>\n",
       "      <td>0.944178</td>\n",
       "      <td>0.341793</td>\n",
       "      <td>0.021087</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>0.001407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.371463</td>\n",
       "      <td>0.040206</td>\n",
       "      <td>0.385898</td>\n",
       "      <td>0.037241</td>\n",
       "      <td>0.007230</td>\n",
       "      <td>0.005267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.016993</td>\n",
       "      <td>0.905910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.016993</td>\n",
       "      <td>0.967410</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.060131</td>\n",
       "      <td>0.975510</td>\n",
       "      <td>0.615380</td>\n",
       "      <td>0.026455</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.978310</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.835980</td>\n",
       "      <td>0.963520</td>\n",
       "      <td>0.161700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  8            38            67            76            78  \\\n",
       "count  40158.000000  40158.000000  40158.000000  40158.000000  40158.000000   \n",
       "mean       0.200639      0.944178      0.341793      0.021087      0.003357   \n",
       "std        0.371463      0.040206      0.385898      0.037241      0.007230   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.016993      0.905910      0.000000      0.005291      0.002146   \n",
       "50%        0.016993      0.967410      0.000000      0.005291      0.002146   \n",
       "75%        0.060131      0.975510      0.615380      0.026455      0.002146   \n",
       "max        1.000000      0.978310      1.000000      0.835980      0.963520   \n",
       "\n",
       "                119  \n",
       "count  40158.000000  \n",
       "mean       0.001407  \n",
       "std        0.005267  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        0.161700  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[best_features].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a closer look at one of the features in particular that show a large discrepancy.  \n",
    "We can compare the training and testing version of feature 38 by plotting them on a histogram.  I am using a log scale, otherwise the smaller bins will not show up on the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fd560b290b8>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fd56081f5f8>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame({'train':X_train['38'], 'test':X_test['38']}).hist(log=True, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above histograms you can clearly see that the training data has a wide range of values and has most values between 0.5 and 0.8.  The training data however has all its values up near 1.0 with a handful at zero.  This is extremely unlikely if both data sets are identically and independantly drawn from the same population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the shape of the distributions more clearly on a KDE plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd55e2f9e48>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame({'train':X_train['38'], 'test':X_test['38']}).plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot it appears that the overall shape is similar but shifted up on the test set.  One possible reason for this could be that the data sets were wrongly scaled.  You could get this effect if it the training set and test set had been scaled independently instead of the scaler being fit to the training data and applied to the test data - an outlier in the training set could push all the other values down after scaling. Unfortunately all the data had been scaled to be between 0 and 1 before we got it and we do not have access to the original data to test this hypothesis.\n",
    "\n",
    "Another indicator that this could be the case is that the maximum value of all features in the test set is never more than 1.0.  If a scaler had been fit to the training set and then applied to the test set it would be likely that a few features in the test set would have a maximum value greater than 1.0 (unless the features were all categorical)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also several features that appear to be categorical with two or three categories, but the categories have been scaled to completely different values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data feature 128 (value, count)\n",
      "[(0.0, 94954), (3.09e-05, 1943), (1.55e-05, 147)]\n",
      "Test data feature 128 (value, count)\n",
      "[(0.0, 38897), (1.0, 1120), (0.5, 141)]\n",
      "\n",
      "Training data feature 129 (value, count)\n",
      "[(0.0, 94954), (1.58e-05, 2090)]\n",
      "Test data feature 129 (value, count)\n",
      "[(0, 38897), (1, 1261)]\n",
      "\n",
      "Training data feature 122 (value, count)\n",
      "[(0.0, 93530), (0.0039683, 2571), (0.007936499999999999, 943)]\n",
      "Test data feature 122 (value, count)\n",
      "[(0.0, 38223), (0.33333, 1423), (0.66667, 512)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in ['128', '129', '122']:\n",
    "    print(f\"Training data feature {col} (value, count)\")\n",
    "    print(list(X_train[col].value_counts().items()))\n",
    "    print(f\"Test data feature {col} (value, count)\")\n",
    "    print(list(X_test[col].value_counts().items()))\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we see that for example feature 128 has three distinct values for both test and training data, but for the training data they take the values 0.0, 0.000015 and 0.00003, while for the test data they take the values 0.0, 0.5 and 1.0.  It is the same for features 129 and 122 (and probably others) - there are orders of magnitude difference in the values taken for the categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get an idea of which features suffer from this problem by looking for features in the test set where the maximum value is more than twice that of the maximum value in the training set, or vice versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_max</th>\n",
       "      <th>test_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.539070</td>\n",
       "      <td>0.040964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.001530</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.994750</td>\n",
       "      <td>0.161700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.004566</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.007936</td>\n",
       "      <td>0.666670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.333330</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.000031</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.000016</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.865560</td>\n",
       "      <td>0.390870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     train_max  test_max\n",
       "80    0.539070  0.040964\n",
       "104   1.000000  0.100000\n",
       "108   0.001530  1.000000\n",
       "110   0.000125  0.750000\n",
       "119   0.994750  0.161700\n",
       "121   0.004566  0.500000\n",
       "122   0.007936  0.666670\n",
       "125   0.333330  1.000000\n",
       "126   0.000015  1.000000\n",
       "127   0.015686  0.500000\n",
       "128   0.000031  1.000000\n",
       "129   0.000016  1.000000\n",
       "144   0.865560  0.390870"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_max = X_train_with_var.max()\n",
    "test_max = X_test_with_var.max()\n",
    "bad_max = (train_max < test_max/2) | (test_max < train_max/2)\n",
    "\n",
    "pd.DataFrame({'train_max': train_max[bad_max], 'test_max': test_max[bad_max]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "It is clear that there are serious discrepancies between the data in the test set and the data in the training set.  This will cause serious problems for anyone trying to build a model using the training data and then validate it against the test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
